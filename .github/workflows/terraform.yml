name: Guest List Terraform Deploy (Dev Only)

on:
  push:
    branches: [gili-feature]
    paths:
      - '**.tf'
      - '**.tfvars'
      - '**.yml'
      - '**.yaml'
      - '.github/workflows/**'
  pull_request:
    branches: [gili-feature]
    paths:
      - '**.tf'
      - '**.tfvars'
      - '**.yml'
      - '**.yaml'
  workflow_dispatch:
    inputs:
      action:
        description: 'Terraform action'
        required: true
        default: 'plan'
        type: choice
        options: [init, plan, apply, destroy]

env:
  TF_VERSION: "1.9.8"
  TF_LOG: INFO
  TF_IN_AUTOMATION: true

  AWS_DEFAULT_REGION: us-east-1

  TF_STATE_BUCKET: guestlist-tfstate-bucket
  TF_LOCK_TABLE: terraform-locks

  CLUSTER_ROLE_NAME: guestlist-cluster-cluster-role
  NODE_ROLE_NAME: guestlist-cluster-node-group-role

jobs:
  terraform:
    runs-on: ubuntu-latest
    environment: dev

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
          audience: sts.amazonaws.com
          output-env-credentials: true

      # Detect existing IAM roles and S3 bucket and export TF_VAR_* for Terraform
      - name: Detect existing IAM/S3 and export TF_VAR_*
        shell: bash
        run: |
          set -euo pipefail

          cluster_role="${CLUSTER_ROLE_NAME}"
          node_role="${NODE_ROLE_NAME}"
          bucket="${TF_STATE_BUCKET}"
          region="${AWS_DEFAULT_REGION}"
          lock_table="${TF_LOCK_TABLE}"

          # Default: don't create IAM in Terraform (use existing), and don't create backend in Terraform
          manage_iam=false
          create_state_backend=false

          # Check IAM roles
          if aws iam get-role --role-name "$cluster_role" >/dev/null 2>&1 && \
             aws iam get-role --role-name "$node_role" >/dev/null 2>&1; then
            echo "Found existing IAM roles: $cluster_role, $node_role"
            manage_iam=false
          else
            echo "One or both IAM roles are missing; Terraform will create them"
            manage_iam=true
          fi

          # Check backend bucket; if missing, create it here (not in Terraform) to avoid backend chicken-and-egg
          if aws s3api head-bucket --bucket "$bucket" >/dev/null 2>&1; then
            echo "Found existing S3 bucket: $bucket"
          else
            echo "Creating S3 bucket: $bucket in $region"
            if [ "$region" = "us-east-1" ]; then
              aws s3api create-bucket --bucket "$bucket" --region "$region"
            else
              aws s3api create-bucket --bucket "$bucket" --region "$region" \
                --create-bucket-configuration LocationConstraint="$region"
            fi
            aws s3api put-bucket-versioning --bucket "$bucket" --versioning-configuration Status=Enabled
            aws s3api put-public-access-block --bucket "$bucket" \
              --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true
            aws s3api put-bucket-encryption --bucket "$bucket" \
              --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
          fi

          # Ensure DynamoDB lock table
          if aws dynamodb describe-table --table-name "$lock_table" >/dev/null 2>&1; then
            echo "Found existing DynamoDB table: $lock_table"
          else
            echo "Creating DynamoDB table: $lock_table"
            aws dynamodb create-table \
              --table-name "$lock_table" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region "$region"
            aws dynamodb wait table-exists --table-name "$lock_table" --region "$region"
          fi

          # Export TF_VAR_* so Terraform can either create or use existing
          echo "TF_VAR_manage_iam=$manage_iam" >> "$GITHUB_ENV"
          echo "TF_VAR_cluster_role_name=$cluster_role" >> "$GITHUB_ENV"
          echo "TF_VAR_node_group_role_name=$node_role" >> "$GITHUB_ENV"

          # Keep backend creation OUT of this stack; backend already exists now
          echo "TF_VAR_create_state_backend=$create_state_backend" >> "$GITHUB_ENV"
          echo "TF_VAR_state_bucket_name=$bucket" >> "$GITHUB_ENV"
          echo "TF_VAR_lock_table_name=$lock_table" >> "$GITHUB_ENV"

          # Region var for provider if you use var.aws_region
          echo "TF_VAR_aws_region=$region" >> "$GITHUB_ENV"

      # Optional: cache Terraform providers to speed up plan
      - name: Enable TF plugin cache env
        run: echo "TF_PLUGIN_CACHE_DIR=${{ runner.temp }}/.terraform-plugin-cache" >> $GITHUB_ENV

      - name: Cache Terraform providers
        uses: actions/cache@v4
        with:
          path: ${{ runner.temp }}/.terraform-plugin-cache
          key: ${{ runner.os }}-tfplugins-${{ hashFiles('**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-tfplugins-

      - name: Detect Terraform Directory
        id: find-dir
        run: |
          if [ -f "./main.tf" ]; then
            echo "dir=." >> $GITHUB_OUTPUT
          elif [ -f "./terraform/main.tf" ]; then
            echo "dir=terraform" >> $GITHUB_OUTPUT
          elif [ -f "./infrastructure/main.tf" ]; then
            echo "dir=infrastructure" >> $GITHUB_OUTPUT
          else
            tfdir=$(find . -name "*.tf" -type f -exec dirname {} \; | head -1)
            echo "dir=${tfdir:-.}" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Init
        working-directory: ${{ steps.find-dir.outputs.dir }}
        run: terraform init -input=false -reconfigure

      - name: Select/Create workspace "dev"
        working-directory: ${{ steps.find-dir.outputs.dir }}
        run: |
          terraform workspace select dev 2>/dev/null || terraform workspace new dev
          terraform workspace show

      - name: Terraform Plan
        if: github.event.inputs.action == 'plan' || github.event_name != 'workflow_dispatch'
        timeout-minutes: 10
        working-directory: ${{ steps.find-dir.outputs.dir }}
        env:
          AWS_RETRY_MODE: adaptive
          AWS_MAX_ATTEMPTS: "5"
        run: |
          set -e
          PLAN_FLAGS="-lock-timeout=60s -parallelism=7 -no-color -input=false -refresh=false"
          if [ -f "dev.tfvars" ]; then
            terraform plan $PLAN_FLAGS -var-file="dev.tfvars" -out=tfplan-dev
          else
            terraform plan $PLAN_FLAGS -var="environment=dev" -out=tfplan-dev
          fi

      - name: Terraform Apply
        if: github.event.inputs.action == 'apply'
        working-directory: ${{ steps.find-dir.outputs.dir }}
        run: |
          if [ -f "tfplan-dev" ]; then
            terraform apply -auto-approve "tfplan-dev"
          elif [ -f "dev.tfvars" ]; then
            terraform apply -auto-approve -var-file="dev.tfvars"
          else
            terraform apply -auto-approve -var="environment=dev"
          fi

      - name: Terraform Destroy
        if: github.event.inputs.action == 'destroy'
        working-directory: ${{ steps.find-dir.outputs.dir }}
        run: |
          if [ -f "dev.tfvars" ]; then
            terraform destroy -auto-approve -var-file="dev.tfvars"
          else
            terraform destroy -auto-approve -var="environment=dev"
          fi
