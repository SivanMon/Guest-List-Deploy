name: Guest List Clean Terraform

on:
  workflow_dispatch:
    inputs:
      action:
        description: Choose Terraform action
        required: true
        type: choice
        options: [plan, apply, destroy]
        default: plan
      target_ref:
        description: Branch or tag to run against (e.g. sivan-feature)
        required: false
        type: string
      environment:
        description: Environment name (sivan, dvir, gili, sahar, dev, staging, prod)
        required: true
        type: choice
        options: [sivan, dvir, gili, sahar, dev, staging, prod]
        default: sivan
      image_tag:
        description: Image tag to deploy (needed for plan/apply)
        required: false
        type: string
      cluster_name:
        description: EKS cluster name (for kubectl and LB check)
        required: false
        type: string
        default: guestlist-cluster
      tf_state_bucket:
        description: Override S3 state bucket (leave empty to auto-derive)
        required: false
        type: string

permissions:
  contents: read

concurrency:
  group: guestlist-tf-${{ github.event.inputs.environment }}
  cancel-in-progress: false

env:
  TF_VERSION: "1.9.8"
  TF_LOG: INFO
  TF_IN_AUTOMATION: "true"
  AWS_REGION: us-east-1

jobs:
  tf:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # allow running the workflow file from one branch but targeting another
          ref: ${{ inputs.target_ref != '' && inputs.target_ref || github.ref }}

      - name: Compute state bucket/key and basics
        id: setup
        shell: bash
        run: |
          set -euo pipefail
          env_short="${{ inputs.environment }}"

          # If you keep per-student buckets with "-feature" suffix for student envs:
          case "$env_short" in
            dev|staging|prod)
              default_bucket="guestlist-tfstate-${env_short}"
              ;;
            *)
              default_bucket="guestlist-tfstate-${env_short}-feature"
              ;;
          esac

          TF_STATE_BUCKET="${{ inputs.tf_state_bucket != '' && inputs.tf_state_bucket || '' }}"
          if [ -z "$TF_STATE_BUCKET" ]; then
            TF_STATE_BUCKET="$default_bucket"
          fi

          TF_STATE_KEY="envs/${env_short}/terraform.tfstate"
          NAMESPACE="guestlist"

          # image_tag only needed for plan/apply
          IMG_TAG="${{ inputs.image_tag }}"
          if [ "${{ inputs.action }}" != "destroy" ] && [ -z "$IMG_TAG" ]; then
            echo "::warning::image_tag is empty; using placeholder 'ignore'"
            IMG_TAG="ignore"
          fi

          echo "env_short=$env_short"       >> "$GITHUB_OUTPUT"
          echo "tf_bucket=$TF_STATE_BUCKET" >> "$GITHUB_OUTPUT"
          echo "tf_key=$TF_STATE_KEY"       >> "$GITHUB_OUTPUT"
          echo "namespace=$NAMESPACE"       >> "$GITHUB_OUTPUT"
          echo "image_tag=$IMG_TAG"         >> "$GITHUB_OUTPUT"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Terraform Init (S3 backend + DynamoDB lock)
        shell: bash
        run: |
          set -euo pipefail
          terraform init -input=false -reconfigure \
            -backend-config="bucket=${{ steps.setup.outputs.tf_bucket }}" \
            -backend-config="key=${{ steps.setup.outputs.tf_key }}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=terraform-locks"

      - name: Terraform Plan
        if: ${{ inputs.action == 'plan' || inputs.action == 'apply' }}
        id: plan
        shell: bash
        env:
          TF_VAR_manage_iam: "false"
          TF_VAR_create_state_backend: "false"
          TF_VAR_cluster_role_name: guestlist-cluster-cluster-role
          TF_VAR_node_group_role_name: guestlist-cluster-node-group-role
          TF_VAR_state_bucket_name: ${{ steps.setup.outputs.tf_bucket }}
        run: |
          set -euo pipefail
          terraform plan -lock-timeout=120s -parallelism=7 -no-color -input=false \
            -var="environment=${{ steps.setup.outputs.env_short }}" \
            -var="aws_region=${AWS_REGION}" \
            -var="namespace=${{ steps.setup.outputs.namespace }}" \
            -var="image_tag=${{ steps.setup.outputs.image_tag }}" \
            -var="aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -out="tfplan-${{ steps.setup.outputs.env_short }}"

      - name: Upload plan artifact (for review)
        if: ${{ inputs.action == 'plan' }}
        uses: actions/upload-artifact@v4
        with:
          name: tfplan-${{ steps.setup.outputs.env_short }}
          path: tfplan-${{ steps.setup.outputs.env_short }}

      - name: Terraform Apply
        if: ${{ inputs.action == 'apply' }}
        shell: bash
        run: |
          set -euo pipefail
          terraform apply -auto-approve "tfplan-${{ steps.setup.outputs.env_short }}"

      - name: Update kubeconfig
        if: ${{ inputs.action == 'apply' }}
        shell: bash
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${{ inputs.cluster_name }}" --region "${AWS_REGION}"

      - name: Show LB URL and health check
        if: ${{ inputs.action == 'apply' }}
        shell: bash
        run: |
          set -euo pipefail
          NS="${{ steps.setup.outputs.namespace }}"
          # Try to resolve any LoadBalancer service tied to the app
          # First by label, then any LB in namespace
          LB_HOST="$(kubectl -n "$NS" get svc -l app=guestlist-api \
                      -o jsonpath='{.items[?(@.spec.type=="LoadBalancer")].status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)"
          if [ -z "$LB_HOST" ]; then
            LB_HOST="$(kubectl -n "$NS" get svc \
                        -o jsonpath='{.items[?(@.spec.type=="LoadBalancer")].status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)"
          fi
          if [ -z "$LB_HOST" ]; then
            echo "::warning::No LoadBalancer hostname found. Services:"
            kubectl -n "$NS" get svc -o wide || true
            exit 0
          fi

          echo "LB hostname: $LB_HOST"
          echo "Attempting health checksâ€¦"

          for p in 80 1111; do
            for path in /health /healthz /ready /readyz /; do
              echo "curl http://$LB_HOST:$p$path"
              if curl -fsS --max-time 5 "http://$LB_HOST:$p$path" ; then
                echo "OK on port $p path $path"
                break 2
              fi
            done
          done

      - name: Terraform Destroy
        if: ${{ inputs.action == 'destroy' }}
        shell: bash
        env:
          TF_VAR_manage_iam: "false"
          TF_VAR_create_state_backend: "false"
          TF_VAR_cluster_role_name: guestlist-cluster-cluster-role
          TF_VAR_node_group_role_name: guestlist-cluster-node-group-role
          TF_VAR_state_bucket_name: ${{ steps.setup.outputs.tf_bucket }}
        run: |
          set -euo pipefail
          terraform destroy -auto-approve -input=false -lock-timeout=120s \
            -var="environment=${{ steps.setup.outputs.env_short }}" \
            -var="aws_region=${AWS_REGION}" \
            -var="namespace=${{ steps.setup.outputs.namespace }}" \
            -var="image_tag=ignore" \
            -var="aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}"
